#!/usr/bin/env python3
"""
Merge Process for Mining Maintenance Records

This script merges duplicate records based on Action Request Number,
handling different field types with appropriate merge strategies.
"""

import argparse
import json
import logging
import sys
from datetime import datetime
from pathlib import Path
from typing import Any, Dict, List, Optional, Union

# Add the project root to Python path
project_root = Path(__file__).parent.parent.parent
sys.path.append(str(project_root))

from dashboard.adapters.config_adapter import ConfigAdapter

# Configure logging
logging.basicConfig(level=logging.INFO, format="%(asctime)s - %(levelname)s - %(message)s")
logger = logging.getLogger(__name__)


class RecordMerger:
    """
    Handles merging of duplicate maintenance records with intelligent field-specific strategies.
    Includes comprehensive difference detection and flexible merge strategies.
    """

    def __init__(self):
        self.config_adapter = ConfigAdapter()

        # Define merge strategies for different field types
        self.merge_strategies = {
            # Core identification fields - keep first occurrence
            "primary_key": ["Action Request Number"],
            # List fields - merge and deduplicate
            "merge_lists": ["Action Plan", "Root Cause", "Asset Number(s)"],
            # String fields that should be concatenated with separator
            "concatenate_strings": ["Comments", "What happened?", "Recurring Comment"],
            # Date fields - keep latest non-null value
            "latest_date": [
                "Completion Date",
                "Response Date",
                "Response Revision Date",
                "Reviewed Date:",
                "Action Plan Verification Date:",
                "Effectiveness Verification Due Date",
            ],
            # Status fields - prioritize most advanced status
            "prioritize_status": ["Stage", "Complete"],
            # Numeric fields - take maximum value
            "max_numeric": ["Days Past Due", "Amount of Loss"],
            # Boolean/Yes-No fields - prioritize 'Yes' over 'No'
            "prioritize_yes": [
                "Did this action plan require a change to the equipment management strategy ?",
                "Did this action plan require a change to the equipment management strategy ? (review)",
                "Is Resp Satisfactory?",
                "IsActionPlanEffective",
            ],
            # Keep first non-null value
            "first_non_null": [
                "Title",
                "Initiation Date",
                "Action Types",
                "Categories",
                "Requested Response Time",
                "Past Due Status",
                "Operating Centre",
                "Init. Dept.",
                "Rec. Dept.",
                "Recurring Problem(s)",
                "Requirement",
                "Obj. Evidence",
                "Recom.Action",
                "Immd. Contain. Action or Comments",
                "Due Date",
                "If yes, are there any corrective actions to update the strategy in APSS, eAM, ASM and BOM as required ?",
                "Reason if not Satisfactory",
                "If yes, APSS Doc #",
                "Asset Activity numbers",
                "Action Plan Eval Comment",
            ],
            # Metadata fields - keep from first record
            "metadata": ["_source_file", "_sheet_name", "_line_number", "_source_path"],
        }

        # Field patterns for automatic strategy detection
        self.field_patterns = {
            "date_patterns": ["date", "time", "due", "completion", "verification", "reviewed"],
            "status_patterns": ["stage", "complete", "status", "satisfactory", "effective"],
            "list_patterns": ["plan", "cause", "number", "action", "asset"],
            "comment_patterns": [
                "comment",
                "description",
                "what happened",
                "requirement",
                "evidence",
            ],
            "numeric_patterns": ["amount", "days", "count", "quantity", "duration", "past due"],
            "boolean_patterns": ["yes", "no", "true", "false", "did", "is", "are", "require"],
        }

        # Status priority order (higher index = more advanced)
        self.stage_priority = [
            "Open - Receiver Proposing Action Plan",
            "Open - Waiting on Receiver to document Root Cause/Action Plan",
            "Action Plan Proposed - Waiting on Action Plan Implementation",
            "Action Plan Implemented - Actions Effective - Closed",
        ]

        self.complete_priority = ["No", "Cancelled", "Moved to long-term plan", "Yes"]

    def detect_field_differences(self, records: List[Dict[str, Any]]) -> Dict[str, Dict[str, Any]]:
        """
        Detect differences between records and analyze field patterns.
        Returns comprehensive difference analysis.
        """
        if len(records) <= 1:
            return {}

        differences = {}
        all_fields = set()

        # Collect all fields from all records
        for record in records:
            all_fields.update(record.keys())

        for field_name in all_fields:
            field_values = [record.get(field_name) for record in records]

            # Normalize values for comparison
            normalized_values = []
            for value in field_values:
                if value is None:
                    normalized_values.append(None)
                elif isinstance(value, list):
                    # Sort lists for consistent comparison
                    normalized_values.append(sorted([str(item) for item in value]))
                elif isinstance(value, str):
                    normalized_values.append(value.strip())
                else:
                    normalized_values.append(value)

            # Check if values differ
            unique_values = []
            for val in normalized_values:
                if val not in unique_values:
                    unique_values.append(val)

            if len(unique_values) > 1:
                differences[field_name] = {
                    "values": field_values,
                    "unique_count": len(unique_values),
                    "has_nulls": None in normalized_values,
                    "data_types": [type(v).__name__ for v in field_values],
                    "merge_strategy": self._determine_merge_strategy(field_name, field_values),
                    "confidence": self._calculate_merge_confidence(field_name, field_values),
                }

        return differences

    def _determine_merge_strategy(self, field_name: str, values: List[Any]) -> str:
        """
        Simplified merge strategy determination: Most fields configured as lists are now strings,
        but we keep the method signature the same for backward compatibility.
        """
        field_lower = field_name.lower()

        # Check explicit strategy mappings first
        for strategy_type, field_list in self.merge_strategies.items():
            if field_name in field_list:
                # Simplify: If it's configured as a list field or a concatenate field,
                # use concatenate_strings consistently
                if strategy_type in ["merge_lists", "concatenate_strings"]:
                    logger.info(f"Field '{field_name}' will use pipe delimiter for merging")
                    return "concatenate_strings"
                return strategy_type

        # Pattern-based detection - simplified for main cases
        if any(pattern in field_lower for pattern in self.field_patterns["date_patterns"]):
            return "latest_date"
        elif any(pattern in field_lower for pattern in self.field_patterns["status_patterns"]):
            return "prioritize_status"
        elif any(pattern in field_lower for pattern in self.field_patterns["list_patterns"]) or any(
            pattern in field_lower for pattern in self.field_patterns["comment_patterns"]
        ):
            # Merge all list-like or comment fields with pipe delimiter
            return "concatenate_strings"
        elif any(pattern in field_lower for pattern in self.field_patterns["numeric_patterns"]):
            return "max_numeric"
        elif any(pattern in field_lower for pattern in self.field_patterns["boolean_patterns"]):
            return "prioritize_yes"

        # Default strategy
        return "first_non_null"

    def _calculate_merge_confidence(self, field_name: str, values: List[Any]) -> float:
        """
        Calculate confidence score for the merge strategy (0.0 to 1.0).
        Higher score means more confident in the merge approach.
        """
        non_null_values = [v for v in values if v is not None]
        if not non_null_values:
            return 1.0  # High confidence for null handling

        # Calculate based on value consistency and field name clarity
        confidence = 0.5  # Base confidence

        # Boost confidence for clear field name patterns
        field_lower = field_name.lower()
        if any(
            pattern in field_lower
            for pattern in self.field_patterns["date_patterns"]
            + self.field_patterns["status_patterns"]
            + self.field_patterns["numeric_patterns"]
        ):
            confidence += 0.3

        # Boost confidence for consistent data types
        data_types = set(type(v).__name__ for v in non_null_values)
        if len(data_types) == 1:
            confidence += 0.2

        # Reduce confidence for highly variable string content
        if all(isinstance(v, str) for v in non_null_values):
            avg_length = sum(len(str(v)) for v in non_null_values) / len(non_null_values)
            if avg_length > 100:  # Long text fields are harder to merge confidently
                confidence -= 0.2

        return min(1.0, max(0.0, confidence))

    def analyze_merge_complexity(self, records: List[Dict[str, Any]]) -> Dict[str, Any]:
        """
        Analyze the complexity of merging the given records.
        """
        if len(records) <= 1:
            return {"complexity": "none", "differences": {}, "recommendations": []}

        differences = self.detect_field_differences(records)

        complexity_score = 0
        high_risk_fields = []
        recommendations = []

        for field_name, diff_info in differences.items():
            # Score based on number of unique values
            complexity_score += diff_info["unique_count"] * 0.1

            # Score based on confidence
            complexity_score += (1.0 - diff_info["confidence"]) * 0.2

            # Identify high-risk fields
            if diff_info["confidence"] < 0.6:
                high_risk_fields.append(field_name)
                recommendations.append(
                    f"Review merge strategy for '{field_name}' (low confidence: {diff_info['confidence']:.2f})"
                )

            if diff_info["unique_count"] > 3:
                recommendations.append(
                    f"Field '{field_name}' has many different values ({diff_info['unique_count']})"
                )

        # Determine overall complexity
        if complexity_score < 0.5:
            complexity_level = "low"
        elif complexity_score < 1.5:
            complexity_level = "medium"
        else:
            complexity_level = "high"

        return {
            "complexity": complexity_level,
            "score": complexity_score,
            "differences": differences,
            "high_risk_fields": high_risk_fields,
            "recommendations": recommendations,
            "field_count": len(differences),
            "record_count": len(records),
        }

    def parse_date(self, date_str: str) -> Optional[datetime]:
        """Parse date string to datetime object."""
        if not date_str or date_str in ["", "None", None]:
            return None

        # Common date formats in the data
        date_formats = [
            "%Y-%m-%d",
            "%d/%m/%Y",
            "%m/%d/%Y",
            "%Y-%m-%d %H:%M:%S",
            "%d/%m/%Y %H:%M:%S",
        ]

        for fmt in date_formats:
            try:
                return datetime.strptime(str(date_str).strip(), fmt)
            except ValueError:
                continue

        logger.warning(f"Could not parse date: {date_str}")
        return None

    def merge_lists(self, values: List[Any]) -> str:
        """Merge list fields (now simplified - deprecated in favor of concatenate_strings)"""
        # Forward to concatenate_strings for consistency
        return self.concatenate_strings(values)

    def concatenate_strings(self, values: List[Any], separator: str = " | ") -> Optional[str]:
        """
        Concatenate string fields with pipe separator, handling both string and list values.
        This is the unified method for merging both list fields and text fields.
        """
        non_null_values = []
        seen = set()

        for value in values:
            if value is None:
                continue

            # Handle all possible value types
            if isinstance(value, list):
                # Direct list values
                items = value
            elif isinstance(value, str):
                # Handle strings that might be semicolon-delimited (from preprocessing)
                if "; " in value:
                    items = value.split("; ")
                else:
                    items = [value]
            else:
                # Other types (numbers, etc.)
                items = [str(value)]

            # Process each item separately to avoid duplicates
            for item in items:
                item_str = str(item).strip()
                if item_str and item_str not in seen:
                    non_null_values.append(item_str)
                    seen.add(item_str)

        return separator.join(non_null_values) if non_null_values else None

    def get_latest_date(self, values: List[Any]) -> Optional[str]:
        """Get the latest date from a list of date values."""
        dates = []
        original_values = []

        for value in values:
            if value is not None and str(value).strip():
                parsed_date = self.parse_date(str(value))
                if parsed_date:
                    dates.append(parsed_date)
                    original_values.append(str(value))

        if not dates:
            return None

        # Return original string format of the latest date
        latest_idx = dates.index(max(dates))
        return original_values[latest_idx]

    def prioritize_status(self, values: List[Any], field_name: str) -> Optional[str]:
        """Prioritize status values based on advancement level."""
        non_null_values = [str(v).strip() for v in values if v is not None and str(v).strip()]

        if not non_null_values:
            return None

        if field_name == "Stage":
            priority_list = self.stage_priority
        elif field_name == "Complete":
            priority_list = self.complete_priority
        else:
            # Default: return first non-null value
            return non_null_values[0]

        # Find the highest priority value
        highest_priority = -1
        best_value = non_null_values[0]

        for value in non_null_values:
            try:
                priority = priority_list.index(value)
                if priority > highest_priority:
                    highest_priority = priority
                    best_value = value
            except ValueError:
                # Value not in priority list, keep if no better option
                continue

        return best_value

    def get_max_numeric(self, values: List[Any]) -> Optional[Union[int, float]]:
        """Get maximum numeric value from list."""
        numeric_values = []

        for value in values:
            if value is not None:
                try:
                    if isinstance(value, (int, float)):
                        numeric_values.append(value)
                    else:
                        numeric_values.append(float(str(value)))
                except (ValueError, TypeError):
                    continue

        return max(numeric_values) if numeric_values else None

    def prioritize_yes(self, values: List[Any]) -> Optional[str]:
        """Prioritize 'Yes' values over 'No' values."""
        non_null_values = [str(v).strip() for v in values if v is not None and str(v).strip()]

        if not non_null_values:
            return None

        # Check for 'Yes' values first
        for value in non_null_values:
            if value.lower() in ["yes", "y", "true", "1"]:
                return value

        # Return first non-null value if no 'Yes' found
        return non_null_values[0]

    def get_first_non_null(self, values: List[Any]) -> Any:
        """Get first non-null value from list."""
        for value in values:
            if value is not None and str(value).strip():
                return value
        return None

    def merge_field(self, field_name: str, values: List[Any]) -> Any:
        """Merge a specific field based on its merge strategy."""
        # Skip primary key fields
        if field_name in self.merge_strategies["primary_key"]:
            return self.get_first_non_null(values)

        # Determine merge strategy dynamically
        strategy = self._determine_merge_strategy(field_name, values)

        # Apply specific merge strategies with simplified logic
        if (
            strategy == "merge_lists"
            or strategy == "concatenate_strings"
            or field_name in self.merge_strategies["merge_lists"]
            or field_name in self.merge_strategies["concatenate_strings"]
        ):
            # Unified approach: all list fields and string fields use concatenate_strings with pipe delimiter
            return self.concatenate_strings(values)
        elif strategy == "latest_date" or field_name in self.merge_strategies["latest_date"]:
            return self.get_latest_date(values)
        elif (
            strategy == "prioritize_status"
            or field_name in self.merge_strategies["prioritize_status"]
        ):
            return self.prioritize_status(values, field_name)
        elif strategy == "max_numeric" or field_name in self.merge_strategies["max_numeric"]:
            return self.get_max_numeric(values)
        elif strategy == "prioritize_yes" or field_name in self.merge_strategies["prioritize_yes"]:
            return self.prioritize_yes(values)
        elif strategy == "first_non_null" or field_name in self.merge_strategies["first_non_null"]:
            return self.get_first_non_null(values)
        elif strategy == "metadata" or field_name in self.merge_strategies["metadata"]:
            return self.get_first_non_null(values)
        else:
            # Default strategy: first non-null value
            logger.warning(
                f"No specific merge strategy for field '{field_name}', using first non-null"
            )
            return self.get_first_non_null(values)

    def merge_duplicate_records(self, records: List[Dict[str, Any]]) -> Dict[str, Any]:
        """Merge a list of duplicate records into a single record."""
        if not records:
            raise ValueError("Cannot merge empty list of records")

        if len(records) == 1:
            return records[0]

        # Analyze merge complexity and differences
        complexity_analysis = self.analyze_merge_complexity(records)
        differences = complexity_analysis["differences"]

        # Log merge complexity
        primary_key = records[0].get("Action Request Number:", "Unknown")
        logger.info(
            f"Merging {len(records)} records for {primary_key} - "
            f"Complexity: {complexity_analysis['complexity']} "
            f"({complexity_analysis['field_count']} differing fields)"
        )

        if complexity_analysis["recommendations"]:
            for rec in complexity_analysis["recommendations"]:
                logger.warning(f"Merge recommendation: {rec}")

        # Get all field names
        all_fields = set()
        for record in records:
            all_fields.update(record.keys())

        merged_record = {}
        merge_decisions = {}

        # Merge each field
        for field_name in all_fields:
            field_values = [record.get(field_name) for record in records]

            # Store merge decision info
            if field_name in differences:
                diff_info = differences[field_name]
                merge_decisions[field_name] = {
                    "strategy": diff_info["merge_strategy"],
                    "confidence": diff_info["confidence"],
                    "unique_values": diff_info["unique_count"],
                    "original_values": field_values,
                }

            merged_record[field_name] = self.merge_field(field_name, field_values)

        # No metadata added to records - keep original fields only

        return merged_record

    def process_data(self, input_file: str, output_file: str) -> Dict[str, Any]:
        """Process the input data file and create merged output."""
        logger.info(f"Loading data from {input_file}")

        data, structure_info = self.load_data(input_file)

        original_count = len(data)
        logger.info(f"Loaded {original_count} records")

        # Group records by Action Request Number
        grouped_records = {}
        for record in data:
            action_request_num = record.get("Action Request Number:")
            if action_request_num:
                if action_request_num not in grouped_records:
                    grouped_records[action_request_num] = []
                grouped_records[action_request_num].append(record)

        # Merge duplicate records
        merged_data = []
        merge_stats = {
            "original_records": original_count,
            "unique_action_requests": len(grouped_records),
            "duplicates_found": 0,
            "duplicates_merged": 0,
            "final_records": 0,
            "complexity_distribution": {"low": 0, "medium": 0, "high": 0},
            "total_differing_fields": 0,
            "high_risk_merges": 0,
            "merge_strategies_used": {},
            "validation_warnings": [],
        }

        for action_request_num, records in grouped_records.items():
            if len(records) > 1:
                merge_stats["duplicates_found"] += len(records) - 1
                merge_stats["duplicates_merged"] += 1

                # Analyze complexity before merging
                complexity_analysis = self.analyze_merge_complexity(records)
                merge_stats["complexity_distribution"][complexity_analysis["complexity"]] += 1
                merge_stats["total_differing_fields"] += complexity_analysis["field_count"]

                if complexity_analysis["high_risk_fields"]:
                    merge_stats["high_risk_merges"] += 1

                # Track merge strategies used
                for field_name, diff_info in complexity_analysis["differences"].items():
                    strategy = diff_info["merge_strategy"]
                    merge_stats["merge_strategies_used"][strategy] = (
                        merge_stats["merge_strategies_used"].get(strategy, 0) + 1
                    )

                logger.info(
                    f"Merging {len(records)} records for Action Request {action_request_num}"
                )

                merged_record = self.merge_duplicate_records(records)
                merged_data.append(merged_record)

                # Validate merge result
                self._validate_merge_result(records, merged_record, merge_stats)
            else:
                merged_data.append(records[0])

        merge_stats["final_records"] = len(merged_data)

        # Save merged data preserving original structure
        logger.info(f"Saving merged data to {output_file}")
        self._save_data(merged_data, structure_info, output_file)

        logger.info(
            f"Merge complete. Reduced {original_count} records to {len(merged_data)} records"
        )
        logger.info(f"Merge statistics: {merge_stats}")

        return merge_stats

    def _validate_merge_result(
        self,
        original_records: List[Dict[str, Any]],
        merged_record: Dict[str, Any],
        stats: Dict[str, Any],
    ) -> None:
        """Validate the merge result and add warnings if issues are detected."""
        primary_key = merged_record.get("Action Request Number:", "Unknown")

        # Check for data loss in critical fields
        critical_fields = ["Action Plan", "Root Cause", "Title", "Stage"]
        for field in critical_fields:
            original_values = [
                rec.get(field) for rec in original_records if rec.get(field) is not None
            ]
            merged_value = merged_record.get(field)

            if original_values and merged_value is None:
                warning = f"Critical field '{field}' lost in merge for {primary_key}"
                logger.warning(warning)
                stats["validation_warnings"].append(warning)

        # Check for unexpected data type changes
        for field_name in merged_record.keys():
            if field_name.startswith("_"):  # Skip metadata fields
                continue

            original_types = set()
            for rec in original_records:
                if rec.get(field_name) is not None:
                    original_types.add(type(rec.get(field_name)).__name__)

            if original_types and len(original_types) == 1:
                original_type = list(original_types)[0]
                merged_type = type(merged_record.get(field_name)).__name__

                if merged_type != original_type and merged_record.get(field_name) is not None:
                    warning = f"Data type changed for '{field_name}' in {primary_key}: {original_type} -> {merged_type}"
                    logger.warning(warning)
                    stats["validation_warnings"].append(warning)

    def analyze_dataset_differences(self, input_file: str) -> Dict[str, Any]:
        """
        Comprehensive analysis of differences in the dataset without merging.
        Useful for understanding data patterns before applying merge strategies.
        """
        logger.info(f"Analyzing dataset differences in {input_file}")

        data = self.load_data(input_file)

        # Group records by Action Request Number
        grouped_records = {}
        for record in data:
            action_request_num = record.get("Action Request Number:")
            if action_request_num:
                if action_request_num not in grouped_records:
                    grouped_records[action_request_num] = []
                grouped_records[action_request_num].append(record)

        # Analyze all duplicates
        analysis_results = {
            "total_records": len(data),
            "unique_action_requests": len(grouped_records),
            "duplicate_groups": 0,
            "field_difference_patterns": {},
            "strategy_recommendations": {},
            "complexity_analysis": {"low": 0, "medium": 0, "high": 0},
            "field_statistics": {},
            "duplicate_examples": [],
        }

        all_differing_fields = set()
        field_merge_strategies = {}

        for action_request_num, records in grouped_records.items():
            if len(records) > 1:
                analysis_results["duplicate_groups"] += 1

                # Analyze this group
                complexity_analysis = self.analyze_merge_complexity(records)
                analysis_results["complexity_analysis"][complexity_analysis["complexity"]] += 1

                # Collect field patterns
                for field_name, diff_info in complexity_analysis["differences"].items():
                    all_differing_fields.add(field_name)

                    if field_name not in analysis_results["field_difference_patterns"]:
                        analysis_results["field_difference_patterns"][field_name] = {
                            "occurrences": 0,
                            "strategies": {},
                            "confidence_scores": [],
                            "unique_value_counts": [],
                        }

                    pattern = analysis_results["field_difference_patterns"][field_name]
                    pattern["occurrences"] += 1
                    pattern["confidence_scores"].append(diff_info["confidence"])
                    pattern["unique_value_counts"].append(diff_info["unique_count"])

                    strategy = diff_info["merge_strategy"]
                    pattern["strategies"][strategy] = pattern["strategies"].get(strategy, 0) + 1
                    field_merge_strategies[field_name] = strategy

                # Store examples for review
                if len(analysis_results["duplicate_examples"]) < 5:
                    analysis_results["duplicate_examples"].append(
                        {
                            "action_request": action_request_num,
                            "record_count": len(records),
                            "complexity": complexity_analysis["complexity"],
                            "differing_fields": list(complexity_analysis["differences"].keys()),
                            "sample_differences": {
                                field: diff_info["values"][:2]  # First 2 values as examples
                                for field, diff_info in list(
                                    complexity_analysis["differences"].items()
                                )[:3]
                            },
                        }
                    )

        # Calculate field statistics
        for field_name in all_differing_fields:
            pattern = analysis_results["field_difference_patterns"][field_name]
            analysis_results["field_statistics"][field_name] = {
                "frequency": pattern["occurrences"],
                "avg_confidence": sum(pattern["confidence_scores"])
                / len(pattern["confidence_scores"]),
                "avg_unique_values": sum(pattern["unique_value_counts"])
                / len(pattern["unique_value_counts"]),
                "recommended_strategy": max(pattern["strategies"], key=pattern["strategies"].get),
            }

        # Generate strategy recommendations
        for field_name, stats in analysis_results["field_statistics"].items():
            if stats["avg_confidence"] < 0.6:
                analysis_results["strategy_recommendations"][field_name] = {
                    "current_strategy": stats["recommended_strategy"],
                    "confidence": stats["avg_confidence"],
                    "recommendation": "Review and potentially customize merge strategy",
                    "reason": f"Low confidence score ({stats['avg_confidence']:.2f})",
                }

        return analysis_results

    def load_data(self, input_file: str) -> tuple[List[Dict[str, Any]], Dict[str, Any]]:
        """Load data from JSON file, handling both flat lists and nested structures.
        Returns tuple of (records, original_structure) to preserve input format."""
        from pathlib import Path

        with open(input_file, "r", encoding="utf-8") as f:
            raw_data = json.load(f)

        # Extract facility name from file path
        facility_name = Path(input_file).stem

        # Store original structure info
        structure_info = {
            "is_direct_array": False,
            "has_records_key": False,
            "has_sheets": False,
            "sheet_name": None,
        }

        # Handle different JSON structures
        records = []
        if isinstance(raw_data, list):
            # Flat list structure
            records = raw_data
            structure_info["is_direct_array"] = True
        elif isinstance(raw_data, dict):
            if "sheets" in raw_data:
                # Multi-sheet structure
                structure_info["has_sheets"] = True
                sheets = raw_data["sheets"]
                if len(sheets) == 1:
                    # Single sheet
                    sheet_name = list(sheets.keys())[0]
                    structure_info["sheet_name"] = sheet_name
                    sheet_data = sheets[sheet_name]

                    if isinstance(sheet_data, dict) and "records" in sheet_data:
                        records = sheet_data["records"]
                        structure_info["has_records_key"] = True
                    elif isinstance(sheet_data, list):
                        records = sheet_data
                    else:
                        raise ValueError(f"Unexpected sheet structure in {sheet_name}")
                else:
                    # Multiple sheets - use first one for now
                    logger.warning(
                        f"Multiple sheets found, using first sheet: {list(sheets.keys())[0]}"
                    )
                    sheet_name = list(sheets.keys())[0]
                    structure_info["sheet_name"] = sheet_name
                    sheet_data = sheets[sheet_name]

                    if isinstance(sheet_data, dict) and "records" in sheet_data:
                        records = sheet_data["records"]
                        structure_info["has_records_key"] = True
                    elif isinstance(sheet_data, list):
                        records = sheet_data
                    else:
                        raise ValueError(f"Unexpected sheet structure in {sheet_name}")
            elif "records" in raw_data:
                # Single object with records array
                records = raw_data["records"]
                structure_info["has_records_key"] = True
            else:
                raise ValueError(
                    "Unrecognized JSON structure - expected list or object with 'sheets' or 'records'"
                )
        else:
            raise ValueError(f"Unexpected data type: {type(raw_data)}")

        # Add facility name to each record
        for record in records:
            if isinstance(record, dict):
                record["_facility_name"] = facility_name

        return records, structure_info

    def _save_data(
        self, merged_data: List[Dict[str, Any]], structure_info: Dict[str, Any], output_file: str
    ) -> None:
        """
        Save merged data in the same structure as the input file.
        This method preserves the original data format whether it was a direct array,
        an object with a 'records' key, or a sheets structure.

        Args:
            merged_data: The list of merged records
            structure_info: Information about the original data structure
            output_file: Path to the output file
        """
        # Format the output based on original structure
        if structure_info.get("is_direct_array", False):
            # Save as a direct array of records
            output_data = merged_data
        elif structure_info.get("has_sheets", False):
            # Save with sheet structure
            sheet_name = structure_info.get("sheet_name", "Sheet1")
            sheet_data = merged_data
            if structure_info.get("has_records_key", False):
                sheet_data = {"records": merged_data}
            output_data = {"sheets": {sheet_name: sheet_data}}
        elif structure_info.get("has_records_key", False):
            # Save with records key
            output_data = {"records": merged_data}
        else:
            # Default format - direct array
            output_data = merged_data

        # Save to output file
        with open(output_file, "w", encoding="utf-8") as f:
            json.dump(output_data, f, indent=2, ensure_ascii=False)

        logger.info(f"Saved {len(merged_data)} merged records to {output_file}")

    def _save_data(
        self, merged_data: List[Dict[str, Any]], structure_info: Dict[str, Any], output_file: str
    ) -> None:
        """
        Save merged data in the same structure as the input file.
        This method preserves the original data format whether it was a direct array,
        an object with a 'records' key, or a sheets structure.

        Args:
            merged_data: The list of merged records
            structure_info: Information about the original data structure
            output_file: Path to the output file
        """
        # Format the output based on original structure
        if structure_info.get("is_direct_array", False):
            # Save as a direct array of records
            output_data = merged_data
        elif structure_info.get("has_sheets", False):
            # Save with sheet structure
            sheet_name = structure_info.get("sheet_name", "Sheet1")
            sheet_data = merged_data
            if structure_info.get("has_records_key", False):
                sheet_data = {"records": merged_data}
            output_data = {"sheets": {sheet_name: sheet_data}}
        elif structure_info.get("has_records_key", False):
            # Save with records key
            output_data = {"records": merged_data}
        else:
            # Default format - direct array
            output_data = merged_data

        # Save to output file
        with open(output_file, "w", encoding="utf-8") as f:
            json.dump(output_data, f, indent=2, ensure_ascii=False)

        logger.info(f"Saved {len(merged_data)} merged records to {output_file}")


def main():
    """Main function to run the merge process."""
    parser = argparse.ArgumentParser(description="Merge duplicate mining maintenance records")
    parser.add_argument(
        "--input-dir",
        default="data/inter_data",
        help="Input directory containing JSON files (relative to project root)",
    )
    parser.add_argument(
        "--input-file", help="Process a single specific input file instead of directory"
    )
    parser.add_argument(
        "--output-dir",
        default="data/facility_data",
        help="Output directory for merged files (relative to project root)",
    )
    parser.add_argument(
        "--merge-all",
        action="store_true",
        help="Merge all facility files into a single output file",
    )
    parser.add_argument(
        "--dry-run", action="store_true", help="Perform dry run without saving output"
    )
    parser.add_argument(
        "--analyze-only", action="store_true", help="Only analyze differences without merging"
    )
    parser.add_argument("--report-file", help="Save analysis report to this file (JSON format)")

    args = parser.parse_args()

    # Convert to absolute paths
    project_root = Path(__file__).parent.parent.parent

    # Determine input files
    input_files = []
    if args.input_file:
        # Process single specific file
        input_file = project_root / args.input_file
        if not input_file.exists():
            logger.error(f"Input file not found: {input_file}")
            sys.exit(1)
        input_files.append(input_file)
    else:
        # Process all JSON files in input directory
        input_dir = project_root / args.input_dir
        if not input_dir.exists():
            logger.error(f"Input directory not found: {input_dir}")
            sys.exit(1)

        # Find all JSON files in the input directory
        input_files = list(input_dir.glob("*.json"))
        if not input_files:
            logger.error(f"No JSON files found in {input_dir}")
            sys.exit(1)

        logger.info(f"Found {len(input_files)} JSON files to process:")
        for file in input_files:
            logger.info(f"  - {file.name}")

    # Setup output directory
    output_dir = project_root / args.output_dir
    output_dir.mkdir(parents=True, exist_ok=True)

    try:
        merger = RecordMerger()

        if args.merge_all and len(input_files) > 1:
            # Merge all facility files into a single output
            logger.info("Merging all facility files into single output")
            all_records = []

            # Load all records from all files
            for input_file in input_files:
                logger.info(f"Loading records from {input_file.name}")
                try:
                    with open(input_file, "r", encoding="utf-8") as f:
                        data = json.load(f)

                    # Handle both list format and nested format
                    if isinstance(data, list):
                        all_records.extend(data)
                    elif isinstance(data, dict) and "facilities" in data:
                        for facility_name, facility_data in data["facilities"].items():
                            if isinstance(facility_data, list):
                                all_records.extend(facility_data)
                            elif isinstance(facility_data, dict) and "records" in facility_data:
                                all_records.extend(facility_data["records"])
                    else:
                        logger.warning(f"Unknown data format in {input_file.name}, skipping")
                        continue

                except Exception as e:
                    logger.error(f"Error loading {input_file.name}: {e}")
                    continue

            # Process merged data
            output_file = output_dir / "mining_maintenance_merged_all.json"

            if args.analyze_only:
                logger.info("Performing analysis-only mode on merged data")
                # Save merged data temporarily for analysis
                temp_file = output_dir / "temp_merged_all.json"
                with open(temp_file, "w", encoding="utf-8") as f:
                    json.dump(all_records, f, indent=2, ensure_ascii=False)

                analysis = merger.analyze_dataset_differences(str(temp_file))
                temp_file.unlink()  # Clean up temp file

                _print_analysis_results(analysis, args, project_root)
                return
            else:
                # Save merged data temporarily then process
                temp_file = output_dir / "temp_merged_all.json"
                with open(temp_file, "w", encoding="utf-8") as f:
                    json.dump(all_records, f, indent=2, ensure_ascii=False)

                if args.dry_run:
                    logger.info("Performing dry run - no output file will be created")
                    stats = merger.process_data(str(temp_file), str(output_file) + ".tmp")
                    # Clean up temp files
                    temp_file.unlink()
                    Path(str(output_file) + ".tmp").unlink()
                else:
                    stats = merger.process_data(str(temp_file), str(output_file))
                    temp_file.unlink()  # Clean up temp file

                _print_merge_results(stats, str(output_file))

        else:
            # Process each file individually
            all_stats = []

            for input_file in input_files:
                logger.info(f"Processing {input_file.name}")

                # Generate output filename - keep same name as input
                output_filename = f"{input_file.stem}.json"
                output_file = output_dir / output_filename

                if args.analyze_only:
                    logger.info(f"Performing analysis-only mode on {input_file.name}")
                    analysis = merger.analyze_dataset_differences(str(input_file))

                    print(f"\n{'='*60}")
                    print(f"ANALYSIS FOR: {input_file.name}")
                    print("=" * 60)
                    _print_analysis_results(analysis, args, project_root, input_file.name)
                    continue

                # Normal merge process
                if args.dry_run:
                    logger.info(f"Performing dry run on {input_file.name}")
                    stats = merger.process_data(str(input_file), str(output_file) + ".tmp")
                    # Remove temp file
                    temp_file_path = Path(str(output_file) + ".tmp")
                    if temp_file_path.exists():
                        temp_file_path.unlink()
                else:
                    stats = merger.process_data(str(input_file), str(output_file))

                stats["input_file"] = input_file.name
                stats["output_file"] = output_file.name
                all_stats.append(stats)

                print(f"\n{'='*50}")
                print(f"COMPLETED: {input_file.name}")
                print("=" * 50)
                _print_merge_results(stats, str(output_file))

            if not args.analyze_only and len(all_stats) > 1:
                # Print summary for multiple files
                print(f"\n{'='*60}")
                print("SUMMARY - ALL FILES PROCESSED")
                print("=" * 60)
                total_original = sum(s["original_records"] for s in all_stats)
                total_final = sum(s["final_records"] for s in all_stats)
                total_reduced = total_original - total_final

                print(f"Files processed: {len(all_stats)}")
                print(f"Total original records: {total_original}")
                print(f"Total final records: {total_final}")
                print(f"Total records reduced: {total_reduced}")
                print("=" * 60)

            # Save merge report to separate file if requested
            if args.report_file:
                report_data = {
                    "timestamp": datetime.now().isoformat(),
                    "files_processed": len(all_stats),
                    "summary": {
                        "total_original_records": sum(s["original_records"] for s in all_stats),
                        "total_final_records": sum(s["final_records"] for s in all_stats),
                        "total_records_reduced": sum(s["original_records"] for s in all_stats)
                        - sum(s["final_records"] for s in all_stats),
                    },
                    "file_details": all_stats,
                }
                report_file_path = project_root / args.report_file
                with open(report_file_path, "w") as f:
                    json.dump(report_data, f, indent=2)
                print(f"Merge report saved to: {report_file_path}")

    except Exception as e:
        logger.error(f"Error during merge process: {e}")
        sys.exit(1)


def _print_analysis_results(analysis, args, project_root, file_name=None):
    """Helper function to print analysis results."""
    prefix = f"[{file_name}] " if file_name else ""

    print(f"{prefix}Total records: {analysis['total_records']}")
    print(f"{prefix}Unique Action Requests: {analysis['unique_action_requests']}")
    print(f"{prefix}Duplicate groups: {analysis['duplicate_groups']}")
    print(
        f"{prefix}Records that would be reduced: {analysis['total_records'] - analysis['unique_action_requests']}"
    )

    print(f"\n{prefix}Complexity Distribution:")
    for complexity, count in analysis["complexity_analysis"].items():
        print(f"  {complexity.capitalize()}: {count} groups")

    print(f"\n{prefix}Top Differing Fields:")
    sorted_fields = sorted(
        analysis["field_statistics"].items(), key=lambda x: x[1]["frequency"], reverse=True
    )
    for field, stats in sorted_fields[:10]:
        print(
            f"  {field}: {stats['frequency']} occurrences, "
            f"strategy: {stats['recommended_strategy']}, "
            f"confidence: {stats['avg_confidence']:.2f}"
        )

    if analysis["strategy_recommendations"]:
        print(f"\n{prefix}Strategy Recommendations:")
        for field, rec in analysis["strategy_recommendations"].items():
            print(f"  {field}: {rec['recommendation']} ({rec['reason']})")

    print(f"\n{prefix}Example Duplicate Groups:")
    for example in analysis["duplicate_examples"]:
        print(
            f"  {example['action_request']}: {example['record_count']} records, "
            f"{example['complexity']} complexity"
        )
        print(f"    Differing fields: {', '.join(example['differing_fields'])}")

    # Save report if requested
    if args.report_file:
        report_filename = args.report_file
        if file_name:
            # Add file-specific suffix to report name
            stem = Path(args.report_file).stem
            suffix = Path(args.report_file).suffix
            report_filename = f"{stem}_{Path(file_name).stem}{suffix}"

        report_path = project_root / report_filename
        report_path.parent.mkdir(parents=True, exist_ok=True)
        with open(report_path, "w", encoding="utf-8") as f:
            json.dump(analysis, f, indent=2, ensure_ascii=False)
        print(f"\n{prefix}Detailed analysis saved to: {report_path}")


def _print_merge_results(stats, output_file_path):
    """Helper function to print merge results."""
    print(f"Original records: {stats['original_records']}")
    print(f"Unique Action Requests: {stats['unique_action_requests']}")
    print(f"Duplicate records found: {stats['duplicates_found']}")
    print(f"Action Requests with duplicates merged: {stats['duplicates_merged']}")
    print(f"Final record count: {stats['final_records']}")
    print(f"Records reduced by: {stats['original_records'] - stats['final_records']}")

    print(f"\nComplexity Distribution:")
    for complexity, count in stats["complexity_distribution"].items():
        print(f"  {complexity.capitalize()}: {count} merges")

    print(f"\nMerge Strategies Used:")
    for strategy, count in stats["merge_strategies_used"].items():
        print(f"  {strategy}: {count} times")

    if stats["validation_warnings"]:
        print(f"\nValidation Warnings ({len(stats['validation_warnings'])}):")
        for warning in stats["validation_warnings"][:5]:  # Show first 5
            print(f"  {warning}")
        if len(stats["validation_warnings"]) > 5:
            print(f"  ... and {len(stats['validation_warnings']) - 5} more")

    if not output_file_path.endswith(".tmp"):
        print(f"\nOutput saved to: {output_file_path}")


if __name__ == "__main__":
    main()
